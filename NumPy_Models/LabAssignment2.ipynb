{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rujuldwivedi/Projects/blob/main/NumPy_Models/LabAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "PK_6JBRw4MZe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing , load_iris, load_digits\n",
        "from sklearn.preprocessing import Normalizer,OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "P0dppdbD4MZg"
      },
      "outputs": [],
      "source": [
        "class MultiplicationLayer :\n",
        "    def __init__(self, X, W) :\n",
        "        self.X = X\n",
        "        self.W = W\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \" An instance of Muliplication Layer.\"\n",
        "\n",
        "    def forward(self):\n",
        "        self.Z = np.dot(self.X, self.W)\n",
        "\n",
        "    def backward(self):\n",
        "        self.dZ_dW = (self.X).T\n",
        "        self.dZ_daZ_prev = self.W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "sJVjC4g84MZv"
      },
      "outputs": [],
      "source": [
        "class BiasAdditionLayer :\n",
        "    def __init__(self, Z : np.ndarray , bias : np.ndarray ):\n",
        "        self.B = bias\n",
        "        self.Z = Z\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of Bias Addition Layer.\"\n",
        "\n",
        "    def forward(self,):\n",
        "        self.Z = self.Z + self.B\n",
        "\n",
        "    def backward(self,):\n",
        "        self.dZ_dB = np.identity( self.B.shape[1] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "naJlrq5I4MZv"
      },
      "outputs": [],
      "source": [
        "class MeanSquaredLossLayer :\n",
        "    def __init__(self, Y : np.ndarray , Y_hat : np.ndarray):\n",
        "        self.Y = Y\n",
        "        self.aZ = Y_hat\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of Mean Squared Loss Layer\"\n",
        "\n",
        "    def forward(self, ):\n",
        "        self.L = np.mean( ( self.aZ - self.Y)**2 )\n",
        "\n",
        "    def backward(self,):\n",
        "        self.dL_daZ = (2/len(self.Y))*(self.aZ - self.Y).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5vljLFe_4MZw"
      },
      "outputs": [],
      "source": [
        "class SoftMaxActivation :\n",
        "    def __init__(self, Z):\n",
        "        self.Z = Z\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of Softmax Activation Layer\"\n",
        "\n",
        "    def forward(self,):\n",
        "        self.aZ = self.softmax(self.Z)\n",
        "\n",
        "    def backward(self,):\n",
        "        self.daZ_dZ = np.diag( self.aZ.reshape(-1) ) - (self.aZ.T)@( (self.aZ))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(Z : np.ndarray):\n",
        "        max_Z = np.max( Z, axis=1 ,keepdims=True )\n",
        "        return (np.exp(Z - max_Z ))/np.sum( np.exp(Z - max_Z), axis=1 , keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "74wrgJ_24MZw"
      },
      "outputs": [],
      "source": [
        "class SigmoidActivation :\n",
        "    def __init__(self,Z ):\n",
        "        self.Z = Z\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of Sigmoid Activation Layer\"\n",
        "\n",
        "    def forward(self,):\n",
        "        self.aZ = self.sigmoid( self.Z )\n",
        "\n",
        "    def backward(self,):\n",
        "        diag_entries = np.multiply(self.aZ, 1-self.aZ).reshape(-1)\n",
        "        self.daZ_dZ = np.diag(diag_entries)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid( Z : np.ndarray ) :\n",
        "        return  1./(1 + np.exp(-Z) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "nuBWJeJy4MZw"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLossLayer :\n",
        "    def __init__(self, Y , Y_pred):\n",
        "        self.Y = Y\n",
        "        self.aZ = Y_pred\n",
        "        self.epsilon = 1e-40\n",
        "\n",
        "\n",
        "    def __str__(self, ):\n",
        "        return \"An instance of Cross Entropy Loss Layer\"\n",
        "\n",
        "    def forward(self, ):\n",
        "        self.L = - np.sum( self.Y * np.log(self.aZ+self.epsilon) )\n",
        "\n",
        "    def backward(self, ):\n",
        "        self.dL_daZ = -1*(self.Y/(self.aZ + self.epsilon)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "5TFWdaLp4MZx"
      },
      "outputs": [],
      "source": [
        "class LinearActivation :\n",
        "    def __init__(self, Z):\n",
        "        self.Z = Z\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of Linear Activation.\"\n",
        "\n",
        "    def forward(self, ):\n",
        "        self.aZ = self.Z\n",
        "\n",
        "    def backward(self,):\n",
        "        self.daZ_dZ = np.identity( self.Z.shape[1] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gBRhwoRy4MZy"
      },
      "outputs": [],
      "source": [
        "class tanhActivation:\n",
        "    def __init__(self, Z):\n",
        "        self.Z = Z\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of tanhActivation class.\"\n",
        "\n",
        "    def forward(self,):\n",
        "        self.aZ = np.tanh(self.Z)\n",
        "\n",
        "    def backward(self,):\n",
        "        self.daZ_dZ = np.diag(1 - self.aZ.reshape(-1)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "l6SNoPHH4MZy"
      },
      "outputs": [],
      "source": [
        "class ReLUActivation:\n",
        "    def __init__(self, Z):\n",
        "        self.Z = Z\n",
        "        self.Leak = 0.01\n",
        "\n",
        "    def __str__(self,):\n",
        "        return \"An instance of ReLU activation\"\n",
        "\n",
        "    def forward(self,):\n",
        "        self.aZ = np.maximum(self.Z,0)\n",
        "\n",
        "    def backward(self,):\n",
        "        self.daZ_dZ = np.diag( [1. if x>=0 else self.Leak for x in self.aZ.reshape(-1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "-rr2xLDy4MZz"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset_name='california',\n",
        "             normalize_X=False,\n",
        "             normalize_y=False,\n",
        "             one_hot_encode_y = False,\n",
        "             test_size=0.2):\n",
        "    if dataset_name == 'california' :\n",
        "        data = fetch_california_housing()\n",
        "    elif dataset_name == 'iris' :\n",
        "        data = load_iris()\n",
        "    elif dataset_name == 'mnist':\n",
        "        data = load_digits()\n",
        "        data['data'] = 1*(data['data']>=8)\n",
        "\n",
        "    X = data['data']\n",
        "    y = data['target'].reshape(-1,1)\n",
        "\n",
        "    if normalize_X == True :\n",
        "        normalizer = Normalizer()\n",
        "        X  = normalizer.fit_transform(X)\n",
        "\n",
        "    if normalize_y == True :\n",
        "        normalizer = Normalizer()\n",
        "        y = normalizer.fit_transform(y)\n",
        "\n",
        "    if one_hot_encode_y == True:\n",
        "        encoder = OneHotEncoder()\n",
        "        y = encoder.fit_transform(y).toarray()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size)\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ykTN-qP64MZz"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, n_inp, n_out, activation_name=\"linear\", seed=42):\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.n_inp = n_inp\n",
        "        self.n_out = n_out\n",
        "\n",
        "        self.X = np.random.random((1, n_inp))\n",
        "        self.Z = np.random.random((1, n_out))\n",
        "\n",
        "        self.W = np.random.random((n_inp, n_out)) * \\\n",
        "            np.sqrt(2 / (n_inp + n_out))\n",
        "        self.B = np.random.random((1, n_out))*np.sqrt(2 / (1 + n_out))\n",
        "\n",
        "        self.multiply_layer = MultiplicationLayer(self.X, self.W)\n",
        "        self.bias_add_layer = BiasAdditionLayer(self.B, self.B)\n",
        "\n",
        "        if activation_name == 'linear':\n",
        "            self.activation_layer = LinearActivation(self.Z)\n",
        "        elif activation_name == 'sigmoid':\n",
        "            self.activation_layer = SigmoidActivation(self.Z)\n",
        "        elif activation_name == 'softmax':\n",
        "            self.activation_layer = SoftMaxActivation(self.Z)\n",
        "        elif activation_name == 'tanh':\n",
        "            self.activation_layer = tanhActivation(self.Z)\n",
        "        elif activation_name == 'relu':\n",
        "            self.activation_layer = ReLUActivation(self.Z)\n",
        "\n",
        "    def forward(self,):\n",
        "        self.multiply_layer.X = self.X\n",
        "        self.multiply_layer.forward()\n",
        "\n",
        "        self.bias_add_layer.Z = self.multiply_layer.Z\n",
        "        self.bias_add_layer.forward()\n",
        "\n",
        "        self.activation_layer.Z = self.bias_add_layer.Z\n",
        "        self.activation_layer.forward()\n",
        "\n",
        "        self.Z = self.activation_layer.aZ\n",
        "\n",
        "    def backward(self,):\n",
        "        self.activation_layer.backward()\n",
        "        self.bias_add_layer.backward()\n",
        "        self.multiply_layer.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Hvz1rvQD4MZ1"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(Layer):\n",
        "    def __init__(self, layers, loss_name=\"mean_squared\", learning_rate=0.01, seed=42):\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.layers = layers\n",
        "        self.n_layers = len(layers)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.inp_shape = self.layers[0].X.shape\n",
        "        self.out_shape = self.layers[-1].Z.shape\n",
        "\n",
        "        self.X = np.random.random(self.inp_shape)\n",
        "        self.Y = np.random.random(self.out_shape)\n",
        "\n",
        "        if loss_name == \"mean_squared\":\n",
        "            self.loss_layer = MeanSquaredLossLayer(self.Y, self.Y)\n",
        "        if loss_name == \"cross_entropy\":\n",
        "            self.loss_layer = CrossEntropyLossLayer(self.Y, self.Y)\n",
        "\n",
        "    def forward(self,):\n",
        "        self.layers[0].X = self.X\n",
        "        self.loss_layer.Y = self.Y\n",
        "\n",
        "        self.layers[0].forward()\n",
        "        for i in range(1, self.n_layers):\n",
        "            self.layers[i].X = self.layers[i-1].Z\n",
        "            self.layers[i].forward()\n",
        "\n",
        "        self.loss_layer.aZ = self.layers[-1].Z\n",
        "        self.loss_layer.forward()\n",
        "\n",
        "    def backward(self,):\n",
        "\n",
        "        self.loss_layer.Z = self.Y\n",
        "        self.loss_layer.backward()\n",
        "        self.grad_nn = self.loss_layer.dL_daZ\n",
        "        for i in range(self.n_layers-1, -1, -1):\n",
        "            self.layers[i].backward()\n",
        "\n",
        "            dL_dZ = np.dot(\n",
        "                self.layers[i].activation_layer.daZ_dZ, self.grad_nn)\n",
        "            dL_dW = np.dot(self.layers[i].multiply_layer.dZ_dW, dL_dZ.T)\n",
        "            dL_dB = np.dot(self.layers[i].bias_add_layer.dZ_dB, dL_dZ).T\n",
        "\n",
        "            self.layers[i].W -= self.learning_rate*dL_dW\n",
        "            self.layers[i].B -= self.learning_rate*dL_dB\n",
        "\n",
        "            self.grad_nn = np.dot(\n",
        "                self.layers[i].multiply_layer.dZ_daZ_prev, dL_dZ)\n",
        "\n",
        "            del dL_dZ, dL_dW, dL_dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "IurtcaLq4MZ2"
      },
      "outputs": [],
      "source": [
        "def createLayers(inp_shape, layers_sizes, layers_activations):\n",
        "    layers = []\n",
        "    n_layers = len(layers_sizes)\n",
        "    layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0])\n",
        "    layers.append(layer_0)\n",
        "    inp_shape_next = layers_sizes[0]\n",
        "    for i in range(1, n_layers):\n",
        "        layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i])\n",
        "        layers.append(layer_i)\n",
        "        inp_shape_next = layers_sizes[i]\n",
        "\n",
        "    out_shape = inp_shape_next\n",
        "    return inp_shape, out_shape, layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9-T57lW04MZ2"
      },
      "outputs": [],
      "source": [
        "def SGD_NeuralNetwork(X_train,\n",
        "                      y_train,\n",
        "                      X_test,\n",
        "                      y_test,\n",
        "                      nn,\n",
        "                      inp_shape=1,\n",
        "                      out_shape=1,\n",
        "                      n_iterations=1000,\n",
        "                      task=\"regression\"\n",
        "                      ):\n",
        "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100)\n",
        "\n",
        "    for iteration, _ in enumerate(iterations):\n",
        "        randomIndx = np.random.randint(len(X_train))\n",
        "\n",
        "        X_sample = X_train[randomIndx, :].reshape(1, inp_shape)\n",
        "        Y_sample = y_train[randomIndx, :].reshape(1, out_shape)\n",
        "\n",
        "        nn.X = X_sample\n",
        "        nn.Y = Y_sample\n",
        "\n",
        "        nn.forward()\n",
        "        nn.backward()\n",
        "\n",
        "    if task == \"regression\":\n",
        "\n",
        "        nn.X = X_train\n",
        "        nn.Y = y_train\n",
        "        nn.forward()\n",
        "        train_error = nn.loss_layer.L\n",
        "\n",
        "        nn.X = X_test\n",
        "        nn.Y = y_test\n",
        "        nn.forward()\n",
        "        test_error = nn.loss_layer.L\n",
        "\n",
        "        if isinstance(nn.loss_layer, MeanSquaredLossLayer):\n",
        "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\" % train_error)\n",
        "            print(\"Mean Squared Loss Error (Test Data)  : %0.5f\" % test_error)\n",
        "\n",
        "    if task == \"classification\":\n",
        "\n",
        "        nn.X = X_train\n",
        "        nn.Y = y_train\n",
        "        nn.forward()\n",
        "        y_true = np.argmax(y_train, axis=1)\n",
        "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
        "        acc = 1*(y_true == y_pred)\n",
        "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(\n",
        "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n",
        "\n",
        "        nn.X = X_test\n",
        "        nn.Y = y_test\n",
        "        nn.forward()\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "        y_pred = np.argmax(nn.loss_layer.aZ, axis=1)\n",
        "        acc = 1*(y_true == y_pred)\n",
        "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(\n",
        "            sum(acc), len(acc), sum(acc)*100/len(acc)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "m5hP95rq4MZ2"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_data('california', normalize_X=True, normalize_y=False, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXGF9R5E4MZ3",
        "outputId": "58b543b2-b062-4331-c7c6-3b93c5b64096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...: 100%|████████████████████████████████████████| 10000/10000 [00:00<00:00, 12921.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Loss Error (Train Data)  : 1.29365\n",
            "Mean Squared Loss Error (Test Data)  : 1.37316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inp_shape = X_train.shape[1]\n",
        "layers_sizes = [1]\n",
        "layers_activations = ['linear']\n",
        "\n",
        "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'mean_squared'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
        "\n",
        "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVDhfQVi4MZ4",
        "outputId": "dfe36dda-d4e6-4acd-e84a-a83d75745cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 4942.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Loss Error (Train Data)  : 1.33544\n",
            "Mean Squared Loss Error (Test Data)  : 1.41885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inp_shape = X_train.shape[1]\n",
        "layers_sizes = [13,1]\n",
        "layers_activations = ['sigmoid','linear']\n",
        "\n",
        "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'mean_squared'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
        "\n",
        "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kL9Y7CC4MZ5",
        "outputId": "dd8f205e-5adb-468f-aa45-13215a19c32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...: 100%|███████████████████████████████████████████| 1000/1000 [00:00<00:00, 3488.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Loss Error (Train Data)  : 1.31529\n",
            "Mean Squared Loss Error (Test Data)  : 1.39820\n"
          ]
        }
      ],
      "source": [
        "inp_shape = X_train.shape[1]\n",
        "layers_sizes = [13,13,1]\n",
        "layers_activations = ['sigmoid','sigmoid','linear']\n",
        "\n",
        "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'mean_squared'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.001)\n",
        "\n",
        "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "jX69Zd2Q4MZ6"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DbkIVeK4MZ7",
        "outputId": "59575873-7ca0-4e4c-8f41-5efdc31d2650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:02<00:00, 4317.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy (Training Data ): 1188/1257 = 94.5107398568019 %\n",
            "Classification Accuracy (Testing Data ): 482/540 = 89.25925925925925 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inp_shape = X_train.shape[1]\n",
        "layers_sizes = [89,10]\n",
        "layers_activations = ['tanh','sigmoid']\n",
        "\n",
        "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'mean_squared'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
        "\n",
        "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFiF0P6C4MZ7",
        "outputId": "8e1810d6-b531-424d-b4ca-f126de2cb8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...: 100%|█████████████████████████████████████████| 10000/10000 [00:01<00:00, 5216.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy (Training Data ): 1203/1257 = 95.70405727923628 %\n",
            "Classification Accuracy (Testing Data ): 496/540 = 91.85185185185185 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "inp_shape = X_train.shape[1]\n",
        "layers_sizes = [89,10]\n",
        "layers_activations = ['tanh','softmax']\n",
        "\n",
        "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'cross_entropy'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
        "\n",
        "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=10000,task=\"classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "5tP5cvR24MZ8"
      },
      "outputs": [],
      "source": [
        "def convolutional_layer(zero_pad_input, l_filter):\n",
        "    inp = zero_pad_input\n",
        "    l = len(inp)\n",
        "    m = len(l_filter)\n",
        "    c = len(zero_pad_input)\n",
        "    s = (c - m) + 1\n",
        "    out = np.zeros((l, l))\n",
        "\n",
        "    for i in range(s):\n",
        "        for j in range(s):\n",
        "            temp = np.zeros((m, m))\n",
        "            row, col = np.indices((m, m))\n",
        "            temp = np.multiply(zero_pad_input[row+i, col+j], l_filter)\n",
        "\n",
        "            out[i][j] = np.sum(temp)\n",
        "    return out\n",
        "\n",
        "def Forward_pass(inp, l_filter):\n",
        "    l = len(inp)\n",
        "    zero_pad_input = np.zeros((l+2, l+2))\n",
        "    zero_pad_input[1:l+1, 1:l+1] = inp\n",
        "\n",
        "    f_out = convolutional_layer(zero_pad_input, l_filter)\n",
        "    return f_out\n",
        "\n",
        "def rotateMatrix(mat):\n",
        "    N = len(mat)\n",
        "    rot_mat = np.zeros((N, N))\n",
        "    k = N - 1\n",
        "    t1 = 0\n",
        "    while (k >= 0 and t1 < 3):\n",
        "        j = N - 1\n",
        "        t2 = 0\n",
        "        while (j >= 0 and t2 < N):\n",
        "            rot_mat[t1][t2] = mat[k][j]\n",
        "            j = j - 1\n",
        "            t2 = t2 + 1\n",
        "        k = k - 1\n",
        "        t1 = t1 + 1\n",
        "\n",
        "    return rot_mat\n",
        "\n",
        "def Backward_pass(inp, output, l_filter):\n",
        "    l = len(inp)\n",
        "    zero_pad_input = np.zeros((l+2, l+2))\n",
        "    zero_pad_input[1:l+1, 1:l+1] = inp\n",
        "\n",
        "    grad_filter = convolutional_layer(zero_pad_input, output)\n",
        "\n",
        "    rotated_filter = rotateMatrix(l_filter)\n",
        "    zero_pad_output = np.zeros((l+2, l+2))\n",
        "    zero_pad_output[1:l+1, 1:l+1] = output\n",
        "    grad_X = convolutional_layer(zero_pad_output, rotated_filter)\n",
        "\n",
        "    return grad_filter, grad_X\n",
        "\n",
        "def flatten(inp_mat):\n",
        "    flatten_vector = []\n",
        "\n",
        "    for i in range(len(inp_mat)):\n",
        "        for j in range(len(inp_mat[0])):\n",
        "            flatten_vector.append(inp_mat[i][j])\n",
        "\n",
        "    flatten_vector = np.array(flatten_vector)\n",
        "    return flatten_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "6jipy3sZ4MZ9"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalLayer:\n",
        "    def __init__(self, inp_shape, activation='tanh', filter_shape=(1, 1), lr=0.01, Co=1, seed=42):\n",
        "\n",
        "        inp = np.random.rand(*inp_shape)\n",
        "        np.random.seed(seed)\n",
        "        assert (inp_shape[1] >= filter_shape[0] and inp_shape[2] >= filter_shape[1]), \\\n",
        "            \"Error : Input {} incompatible with filter {}\".format(\n",
        "                inp.shape, filter_shape)\n",
        "\n",
        "        self.inp = np.random.rand(*inp_shape)\n",
        "        self.inp_shape = inp_shape\n",
        "\n",
        "        self.Ci = self.inp.shape[0]\n",
        "        self.Co = Co\n",
        "        self.filters_shape = (self.Co, self.Ci,  *filter_shape)\n",
        "        self.out_shape = (self.Co, self.inp.shape[1] - filter_shape[0] + 1, self.inp.shape[2] - filter_shape[1] + 1)\n",
        "        self.flatten_shape = np.prod(self.out_shape)\n",
        "        self.lr = lr\n",
        "\n",
        "        self.filters = np.random.rand(*self.filters_shape)\n",
        "        self.biases = np.random.rand(*self.out_shape)\n",
        "        self.out = np.random.rand(*self.out_shape)\n",
        "        self.flatten_out = np.random.rand(1, self.flatten_shape)\n",
        "\n",
        "        if activation == 'tanh':\n",
        "            self.activation_layer = tanhActivation(self.out)\n",
        "\n",
        "    def forward(self, ):\n",
        "        self.out = np.copy(self.biases)\n",
        "        for i in range(self.Co):\n",
        "            for j in range(self.Ci):\n",
        "                self.out[i] += self.convolve(self.inp[j], self.filters[i, j])\n",
        "\n",
        "        self.flatten()\n",
        "        self.activation_layer.Z = self.flatten_out\n",
        "        self.activation_layer.forward()\n",
        "\n",
        "    def backward(self, grad_nn):\n",
        "\n",
        "        self.activation_layer.backward()\n",
        "        loss_gradient = np.dot(self.activation_layer.daZ_dZ, grad_nn)\n",
        "        loss_gradient = np.reshape(loss_gradient, self.out_shape)\n",
        "\n",
        "        self.filters_gradient = np.zeros(self.filters_shape)\n",
        "        self.input_gradient = np.zeros(self.inp_shape)\n",
        "        self.biases_gradient = loss_gradient\n",
        "        padded_loss_gradient = np.pad(loss_gradient, ((0, 0), (self.filters_shape[2]-1, self.filters_shape[2]-1), (self.filters_shape[3]-1, self.filters_shape[3]-1)))\n",
        "\n",
        "        for i in range(self.Co):\n",
        "            for j in range(self.Ci):\n",
        "                self.filters_gradient[i, j] = self.convolve(self.inp[j], loss_gradient[i])\n",
        "                rot180_Kij = np.rot90(np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))\n",
        "                self.input_gradient[j] += self.convolve(padded_loss_gradient[i], rot180_Kij)\n",
        "\n",
        "        self.filters -= self.lr*self.filters_gradient\n",
        "        self.biases -= self.lr*self.biases_gradient\n",
        "\n",
        "    def flatten(self, ):\n",
        "        self.flatten_out = self.out.reshape(1, -1)\n",
        "        x_conv_y = np.zeros((x.shape[0] - y.shape[0] + 1, x.shape[1] - y.shape[1] + 1))\n",
        "        for i in range(x.shape[0]-y.shape[0] + 1):\n",
        "            for j in range(x.shape[1] - y.shape[1] + 1):\n",
        "                tmp = x[i:i+y.shape[0], j:j+y.shape[1]]\n",
        "                tmp = np.multiply(tmp, y)\n",
        "                x_conv_y[i, j] = np.sum(tmp)\n",
        "        return x_conv_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "37T15oes4MZ9"
      },
      "outputs": [],
      "source": [
        "class CNN :\n",
        "    def __init__(self,  convolutional_layer, nn, seed = 42):\n",
        "        self.nn = nn\n",
        "        self.convolutional_layer = convolutional_layer\n",
        "        self.X = np.random.rand(*self.convolutional_layer.inp_shape)\n",
        "        self.Y = np.random.rand(*self.nn.out_shape)\n",
        "\n",
        "    def forward(self,):\n",
        "        self.convolutional_layer.inp = self.X\n",
        "        self.convolutional_layer.forward()\n",
        "\n",
        "        self.nn.X = self.convolutional_layer.activation_layer.aZ\n",
        "        self.nn.Y = self.Y\n",
        "        self.nn.forward()\n",
        "\n",
        "    def backward(self,):\n",
        "        self.nn.backward()\n",
        "        self.convolutional_layer.backward( self.nn.grad_nn )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "M6WAyd5J4MZ-"
      },
      "outputs": [],
      "source": [
        "def SGD_CNN(X_train,\n",
        "            y_train,\n",
        "            X_test,\n",
        "            y_test,\n",
        "            cnn,\n",
        "            inp_shape,\n",
        "            out_shape,\n",
        "            n_iterations=1000,\n",
        "            task=\"classification\"): # This function is used to train the convolutional neural network model using stochastic gradient descent\n",
        "\n",
        "    iterations = trange(n_iterations, desc=\"Training ...\", ncols=100) # progress bar\n",
        "\n",
        "    for iteration, _ in enumerate(iterations): # train the model for each iteration\n",
        "        randomIndx = np.random.randint(len(X_train)) # randomly choose a sample subset of the data\n",
        "        X_sample = X_train[randomIndx, :].reshape(inp_shape) # input data\n",
        "        Y_sample = y_train[randomIndx, :].reshape(out_shape) # output data\n",
        "\n",
        "        cnn.X = X_sample # initialize the input data to the sample subset of the data\n",
        "        cnn.Y = Y_sample # initialize the output data to the sample subset of the data\n",
        "\n",
        "        cnn.forward()  # Forward Pass\n",
        "        cnn.backward()  # Backward Pass\n",
        "\n",
        "    # We'll run only forward pass for train and test data and check accuracy/error because we have already updated the weights and biases in the backward pass\n",
        "\n",
        "    if task == \"classification\": # check the accuracy for classification problems\n",
        "        X_train = X_train.reshape(-1, 8, 8) # reshape the input data\n",
        "        y_true = np.argmax(y_train, axis=1) # true output\n",
        "        acc = 0 # accuracy\n",
        "        for i in range(len(X_train)): # for each sample in the training data\n",
        "            cnn.X = X_train[i][np.newaxis, :, :] # input data\n",
        "            cnn.Y = y_train[i] # true output\n",
        "            cnn.forward() # forward pass of the convolutional neural network\n",
        "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1) # predicted output\n",
        "            if (y_pred_i == y_true[i]): # check if the predicted output is equal to the true output\n",
        "                acc += 1 # increment the accuracy\n",
        "\n",
        "        print(\"Classification Accuracy (Training Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" ) #str is used to convert the output to a string\n",
        "\n",
        "        X_test = X_test.reshape(-1, 8, 8) # reshape the input data\n",
        "        y_true = np.argmax(y_test, axis=1) # true output\n",
        "        acc = 0 # accuracy\n",
        "        for i in range(len(X_test)): # for each sample in the testing data\n",
        "            cnn.X = X_test[i][np.newaxis, :, :] # input data\n",
        "            cnn.Y = y_test[i] # true output\n",
        "            cnn.forward() # forward pass of the convolutional neural network\n",
        "            y_pred_i = np.argmax(cnn.nn.loss_layer.aZ, axis=1) # predicted output\n",
        "            if (y_pred_i == y_true[i]): # check if the predicted output is equal to the true output\n",
        "                acc += 1 # increment the accuracy\n",
        "\n",
        "        print(\"Classification Accuracy (Testing Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" ) #str is used to convert the output to a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "fJgbqUxy4MZ-"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_data('mnist', one_hot_encode_y=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "GQuyybdm4MaG",
        "outputId": "0cf9871d-2681-4422-9bab-14af99ab7a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training ...:   0%|                                                        | 0/5000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ConvolutionalLayer' object has no attribute 'convolve'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-384b128b3b86>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mout_shape\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mSGD_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_inp_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-89-5400438dabf5>\u001b[0m in \u001b[0;36mSGD_CNN\u001b[0;34m(X_train, y_train, X_test, y_test, cnn, inp_shape, out_shape, n_iterations, task)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_sample\u001b[0m \u001b[0;31m# initialize the output data to the sample subset of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-596807876615>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-eac788a8a98d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ConvolutionalLayer' object has no attribute 'convolve'"
          ]
        }
      ],
      "source": [
        "conv_inp_shape = (1,8,8)\n",
        "Co = 16\n",
        "conv_filter_shape = (3,3)\n",
        "conv_activation = 'tanh'\n",
        "convolutional_layer = ConvolutionalLayer(conv_inp_shape,  filter_shape = conv_filter_shape,  Co = Co,  activation = conv_activation, lr = 0.01)\n",
        "nn_inp_shape = convolutional_layer.flatten_shape\n",
        "layers_sizes = [10]\n",
        "layers_activations = ['softmax']\n",
        "\n",
        "nn_inp_shape, nn_out_shape, layers = createLayers(nn_inp_shape, layers_sizes, layers_activations)\n",
        "loss_nn = 'cross_entropy'\n",
        "\n",
        "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
        "\n",
        "cnn = CNN( convolutional_layer, nn)\n",
        "out_shape =  (1, layers_sizes[-1])\n",
        "\n",
        "SGD_CNN(X_train,y_train,X_test,y_test, cnn,conv_inp_shape, out_shape,n_iterations=5000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}